{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oczAvZviIqnU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/godwinkhalko/miniconda3/envs/base_1/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import accuracy_score\n",
        "import warnings\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50\n",
        "from PIL import Image\n",
        "from timm.models.vision_transformer import VisionTransformer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import joblib\n",
        "from tqdm import tqdm\n",
        "warnings.filterwarnings(\"ignore\", message=\"numerical errors at iteration 0\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSjZycBFIC2u"
      },
      "outputs": [],
      "source": [
        "# Semantic Segmentation Model\n",
        "class SemanticSegmentor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.segmentation_model = deeplabv3_resnet50(pretrained=True)\n",
        "        # self.segmentation_model.eval()  # Freeze weights\n",
        "\n",
        "    def forward(self, x):\n",
        "        with torch.no_grad():\n",
        "            seg_map = self.segmentation_model(x)['out']\n",
        "        return seg_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSk3hiq9Wiq0"
      },
      "outputs": [],
      "source": [
        "#Multimodal Feature Fusion (MFF)\n",
        "class MultimodalFusion(nn.Module):\n",
        "    def __init__(self, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.projection = nn.Linear(embed_dim, embed_dim)  # f(.) projection\n",
        "        self.back_projection = nn.Linear(embed_dim, embed_dim)  # g(.) back-projection\n",
        "\n",
        "        # Attention module\n",
        "        self.attention_mlp = nn.Sequential(\n",
        "            nn.Linear(2 * embed_dim, embed_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embed_dim, 2),\n",
        "            nn.Softmax(dim=1)  # Generates [w_rgb, w_seg]\n",
        "        )\n",
        "\n",
        "    def forward(self, rgb_cls, seg_cls):\n",
        "        # Compute modality attention\n",
        "        att_input = torch.cat([rgb_cls, seg_cls], dim=1)\n",
        "        weights = self.attention_mlp(att_input)  # [batch, 2]\n",
        "        w_rgb, w_seg = weights[:, 0].unsqueeze(1), weights[:, 1].unsqueeze(1)\n",
        "\n",
        "        # Weighted CLS token fusion (final layer)\n",
        "        rgb_final = (1 + w_rgb) * rgb_cls\n",
        "        seg_final = (1 + w_seg) * seg_cls\n",
        "        fmm = torch.cat([rgb_final, seg_final], dim=1)  # Final fused feature\n",
        "\n",
        "        return fmm\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCrZMwadMXhr"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, dim_model: int, dropout_p: float = 0.1, max_len: int=1000):\n",
        "        \"\"\"Initializes the positional embedding layer to enrich data fed into transformers\n",
        "           with positional information.\n",
        "        Args:\n",
        "            dim_model (int): model dimension\n",
        "            dropout_p (float, optional): dropout for all embeddings. Defaults to 0.1.\n",
        "            max_len (int, optional): determines how far the position can influence other tokens. Defaults to 1000.\n",
        "        Note:\n",
        "            This code is a modified version of: `<https://pytorch.org/tutorials/beginner/transformer_tutorial.html>`_.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        # Encoding\n",
        "        pos_encoding = torch.zeros(max_len, dim_model)\n",
        "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1)\n",
        "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model)\n",
        "\n",
        "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
        "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
        "\n",
        "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
        "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
        "\n",
        "        # Saving buffer (same as parameter without gradients needed)\n",
        "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_parameter('pos_encoding', nn.Parameter(pos_encoding, requires_grad=False))\n",
        "\n",
        "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
        "        \"\"\"Generates positional embeddings.\n",
        "        Args:\n",
        "            token_embedding (torch.tensor): original embeddings\n",
        "        Returns:\n",
        "            torch.tensor: transformed embeddings\n",
        "        \"\"\"\n",
        "        # Residual connection + positional encoding\n",
        "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jU_jM1rbuH4"
      },
      "outputs": [],
      "source": [
        "class MultimodalTransformer(nn.Module):\n",
        "    def __init__(self, img_size, patch_size = 8, vit_model: str = \"vit_base_patch16_224\", embed_dim: int = 768, num_layers: int = 3):\n",
        "        super(MultimodalTransformer, self).__init__()\n",
        "\n",
        "        # Load pre-trained ViT models for RGB and Semantic maps\n",
        "        self.rgb_transformer = VisionTransformer(img_size=img_size, patch_size=patch_size, embed_dim=embed_dim, depth=num_layers, num_heads=6, mlp_ratio=4)\n",
        "        self.seg_transformer = VisionTransformer(img_size=img_size, patch_size=patch_size, embed_dim=embed_dim, in_chans=1, depth=num_layers, num_heads=6, mlp_ratio=4)\n",
        "\n",
        "        # Projection layers to align CLS token dimensions\n",
        "        self.cls_projection = nn.Linear(embed_dim, embed_dim)  # f(.) projection\n",
        "        self.back_projection = nn.Linear(embed_dim, embed_dim)  # g(.) back-projection\n",
        "\n",
        "        # Attention fusion of CLS token from the last layer\n",
        "        self.fusion_module = MultimodalFusion()\n",
        "\n",
        "        #Positional Encoder\n",
        "        self.pos_encoder = PositionalEncoder(dim_model=embed_dim)\n",
        "\n",
        "\n",
        "    def forward(self, rgb_input, seg_input):\n",
        "        # Extract embeddings from both transformers\n",
        "        rgb_tokens = self.rgb_transformer.patch_embed(rgb_input)\n",
        "        seg_tokens = self.seg_transformer.patch_embed(seg_input)\n",
        "\n",
        "        cls_rgb = self.rgb_transformer.cls_token.expand(rgb_tokens.shape[0], -1, -1)\n",
        "        cls_seg = self.seg_transformer.cls_token.expand(seg_tokens.shape[0], -1, -1)\n",
        "\n",
        "        # Positional embedding\n",
        "        #Concat the cls token for resective tokens(rgb or seg)\n",
        "        rgb_tokens = torch.cat([cls_rgb, rgb_tokens], dim=1)\n",
        "        seg_tokens = torch.cat([cls_seg, seg_tokens], dim=1)\n",
        "        #add the positional embeddings\n",
        "        rgb_tokens = self.pos_encoder(rgb_tokens)\n",
        "        seg_tokens = self.pos_encoder(seg_tokens)\n",
        "\n",
        "        for layer in range(len(self.rgb_transformer.blocks)):\n",
        "            rgb_tokens = self.rgb_transformer.blocks[layer](rgb_tokens)\n",
        "            seg_tokens = self.seg_transformer.blocks[layer](seg_tokens)\n",
        "\n",
        "            # Extract CLS tokens after each layer\n",
        "            cls_rgb = self.cls_projection(rgb_tokens[:, 0])\n",
        "            cls_seg = self.cls_projection(seg_tokens[:, 0])\n",
        "\n",
        "            # Sum CLS tokens and append back to patch tokens\n",
        "            fused_cls = self.back_projection(cls_rgb + cls_seg)\n",
        "            rgb_tokens = torch.cat([fused_cls.unsqueeze(1), rgb_tokens[:, 1:]], dim=1)\n",
        "            seg_tokens = torch.cat([fused_cls.unsqueeze(1), seg_tokens[:, 1:]], dim=1)\n",
        "\n",
        "        # Final CLS tokens from last layer\n",
        "        cls_rgb = rgb_tokens[:, 0]\n",
        "        cls_seg = seg_tokens[:, 0]\n",
        "\n",
        "        # Attention fusion of the cls tokens from the last layer\n",
        "        fmm = self.fusion_module(cls_rgb, cls_seg)\n",
        "\n",
        "        return fmm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5pNS7XssWg2"
      },
      "outputs": [],
      "source": [
        "#Define a mlp layer that predict a class based on the input\n",
        "class mlp(nn.Module):\n",
        "  def __init__(self, input_dim = 1536, output_dim = 10):\n",
        "    super(mlp, self).__init__()\n",
        "    self.fc1 = nn.Linear(input_dim, 1024)\n",
        "    self.fc2 = nn.Linear(1024, 2048)\n",
        "    self.fc3 = nn.Linear(2048, output_dim)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc3(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aINp2nc5qL4N"
      },
      "outputs": [],
      "source": [
        "#Define a classifier for based on the Semantic Segmentor and multimodal model.\n",
        "class MultiModalClassifier(nn.Module):\n",
        "  def __init__(self, input_dim = 256,feature_size = 1536,  output_dim = 2781):\n",
        "    super(MultiModalClassifier, self).__init__()\n",
        "    self.sematic_segmantic = SemanticSegmentor()\n",
        "    self.multimodal = MultimodalTransformer(input_dim)\n",
        "    self.mlp = mlp(feature_size, output_dim)\n",
        "\n",
        "  # x is the rgb image of the shape (batch_size, channels, height, width)\n",
        "  def forward(self, x):\n",
        "    #Generate the semantic maps for the input images\n",
        "\n",
        "    semantic_output = self.sematic_segmantic(x)\n",
        "    semantic_map = torch.argmax(semantic_output.squeeze(), dim=1).unsqueeze(1).float()\n",
        "\n",
        "    # Pass the semantic map and the rgb images through the MultimodalTransformer\n",
        "    multimodal_output = self.multimodal(x, semantic_map)# (batch, 1536)\n",
        "\n",
        "    # pass throught the mlp to get the classes\n",
        "    output = self.mlp(multimodal_output)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99dNXKhPuVw2"
      },
      "source": [
        "## Working with the Data\n",
        "Prepare the data for the training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def denormalize(img_tensor):\n",
        "    \"\"\"Reverse normalization using ImageNet stats\"\"\"\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "    return img_tensor * std + mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ImageLabelDataset(Dataset):\n",
        "    def __init__(self, image_dir, dataframe, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataframe.iloc[idx]\n",
        "        img_filename = row['filename']\n",
        "        label = row['polygon_label']\n",
        "        img_path = os.path.join(self.image_dir, img_filename)\n",
        "        \n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        \n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        label = torch.tensor(label, dtype=torch.long)\n",
        "        \n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getDataset():\n",
        "    img_path = '/home/godwinkhalko/DLCV/00'\n",
        "    label_path = '/home/godwinkhalko/DLCV/labelled_points.xlsx'\n",
        "    \n",
        "    df = pd.read_excel(label_path, dtype={'id': str})\n",
        "    \n",
        "    image_files = os.listdir(img_path)\n",
        "    id_to_filename = {}\n",
        "    for f in image_files:\n",
        "        id_ = os.path.splitext(f)[0] \n",
        "        id_to_filename[id_] = f\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "\n",
        "    df['polygon_label'] = label_encoder.fit_transform(df['polygon_label'])\n",
        "\n",
        "    joblib.dump(label_encoder, 'label_encoder.pkl')\n",
        "\n",
        "    filtered_df = df[df['id'].isin(id_to_filename.keys())]\n",
        "\n",
        "\n",
        "    filtered_df['filename'] = filtered_df['id'].map(id_to_filename)\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    dataset = ImageLabelDataset(image_dir=img_path, dataframe=filtered_df, transform=transform)\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def showImages(dataloader):\n",
        "    images, labels = next(iter(dataloader))\n",
        "    \n",
        "    fig, axs = plt.subplots(1, 5, figsize=(15, 3))\n",
        "\n",
        "    for i in range(5):\n",
        "        image = denormalize(images[i]).cpu().numpy()\n",
        "        image = np.transpose(image, (1, 2, 0))\n",
        "        image = np.clip(image, 0, 1)\n",
        "        \n",
        "        axs[i].imshow(image)\n",
        "        axs[i].set_title(f\"Label: {labels[i].item()}\")\n",
        "        axs[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v74Ks_29zN5"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYqY5bOO_ubQ"
      },
      "outputs": [],
      "source": [
        "def train(model, dataset, epochs, batch_size, optimizer, criterion, save_point = 500):\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Define the Dataloader\n",
        "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "    train_loss = []\n",
        "    train_accuracy = []\n",
        "    checkpoint_path = \"/home/godwinkhalko/DLCV/checkpoint.pth\"\n",
        "    modified_checkpoint_path = \"/home/godwinkhalko/DLCV/mod_checkpoint.pth\"\n",
        "    # print(f\"Checking if a checkpoint exists\")\n",
        "    # if os.path.exists(checkpoint_path):\n",
        "\n",
        "    #     print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
        "    #     checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    #     model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        \n",
        "    #     if 'optimizer_state_dict' in checkpoint:\n",
        "    #         optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    #     start_epoch = checkpoint.get('epoch', 0) + 1\n",
        "    #     print(f\"Checkpoint loaded, resuming from epoch {start_epoch}\")\n",
        "    # else:\n",
        "    #     print(\"No checkpoint found, starting from scratch.\")\n",
        "    #     start_epoch = 0\n",
        "\n",
        "    print(\"Started Training\")\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # ✅ Ensure model is in training mode\n",
        "        optimizer.zero_grad()  # ✅ Reset gradients before batch loop\n",
        "\n",
        "        training_loss_batch = []\n",
        "        training_accuracies_batch = []\n",
        "\n",
        "        print(f\"Started training for {epoch + 1}\")\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()  # ✅ Reset gradients for every batch\n",
        "\n",
        "            # Forward pass\n",
        "            output= model(data)\n",
        "\n",
        "            loss = criterion(output, target) \n",
        "\n",
        "            training_loss_batch.append(loss.item())\n",
        "\n",
        "            # Backpropagation\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()  # ✅ Update weights\n",
        "\n",
        "            # Calculate accuracy\n",
        "            predicted_classes = torch.argmax(output, dim=1)\n",
        "            accuracy = accuracy_score(target.cpu().numpy(), predicted_classes.cpu().numpy())\n",
        "            training_accuracies_batch.append(accuracy)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item()}, Accuracy: {accuracy}\", end=\"\\r\")\n",
        "            \n",
        "            if batch_idx % save_point == 0:\n",
        "                torch.save({\n",
        "                        'epoch': epoch,\n",
        "                        'model_state_dict': model.state_dict(),\n",
        "                        'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    }, modified_checkpoint_path)\n",
        "        # Average training loss and accuracy\n",
        "        train_loss.append(np.mean(training_loss_batch))\n",
        "        train_accuracy.append(np.mean(training_accuracies_batch))\n",
        "        print(f\"\\n Epoch {epoch+1}/{epochs}, Training Loss: {np.mean(training_loss_batch)}, Training Accuracy: {np.mean(training_accuracies_batch)}\")\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UGK96b2xH-I",
        "outputId": "5d847d41-1a39-4cf9-8007-bb3c8a789229"
      },
      "outputs": [],
      "source": [
        "#Initialize the training parameters\n",
        "epochs = 100\n",
        "batch_size = 16\n",
        "learning_rate = 1e-4\n",
        "weight_decay = 1e-5\n",
        "\n",
        "#Initlaize the models and stuff\n",
        "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MultiModalClassifier()\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train_dataset = getDataset()\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "zHkbL7lmxVIq",
        "outputId": "44ef47d1-6aa5-4d23-cb8f-213fd66a115a"
      },
      "outputs": [],
      "source": [
        "#Run the training\n",
        "trained_model = train(model=model,\n",
        "                        dataset=train_dataset,\n",
        "                        epochs=100,\n",
        "                        batch_size=batch_size,\n",
        "                        optimizer=optimizer,\n",
        "                        criterion=criterion,\n",
        "                        save_point=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "del model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base_1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
